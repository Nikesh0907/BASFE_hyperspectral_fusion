{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HChAL6ob6UZh"
   },
   "source": [
    "# CAVE / Generic Hyperspectral-Multispectral Fusion Notebook\n",
    "\n",
    "This notebook adapts the BASFE model (model cell left untouched) to work on a dataset laid out like:\n",
    "\n",
    "```\n",
    "root/\n",
    "  test/\n",
    "    X/  (LR-HSI *.mat)\n",
    "    Y/  (HR-MSI *.mat)\n",
    "  Z/\n",
    "    train/\n",
    "      X/       (HR-HSI ground-truth)\n",
    "      X_blur/  (LR-HSI versions matched to X)\n",
    "      Y/       (HR-MSI)\n",
    "```\n",
    "\n",
    "You may change the directory names in the configuration cell. The code will:\n",
    "- Load training scenes (HR-HSI, LR-HSI, HR-MSI)\n",
    "- Extract aligned patches\n",
    "- Train the BASFE model\n",
    "- Run inference on test scenes (LR-HSI + HR-MSI) and (optionally) compute metrics if HR-HSI GT is available\n",
    "\n",
    "Assumptions (can be adjusted in config cell):\n",
    "- Each `.mat` file contains at least one 3D array (H, W, Bands). The first suitable array is selected automatically.\n",
    "- Value ranges are normalized to [0,1].\n",
    "- Scale factor is inferred from HR vs LR spatial dimensions in training set (integer ratio).\n",
    "\n",
    "Modify `CONFIG` dict below to fit your filesystem.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WGwEd5Oi7SwD"
   },
   "source": [
    "# Configuration & Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7ZWv58ER5hzZ"
   },
   "outputs": [],
   "source": [
    "# If running in Colab uncomment the following two lines.\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')\n",
    "\n",
    "import os, sys, math, random, json, glob, time, socket, platform, shutil\n",
    "import numpy as np\n",
    "import cv2 as cv\n",
    "import scipy.io as sio\n",
    "from scipy.io import savemat\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import pyplot\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras import layers\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "print('TensorFlow version:', tf.__version__)\n",
    "\n",
    "# ---------------- Configuration ----------------\n",
    "# Added performance tuning knobs for large datasets.\n",
    "CONFIG = {\n",
    "    'ROOT_DIR': '/kaggle/input/cave-dataset',   # CHANGE IF DIFFERENT\n",
    "    'TRAIN_HR_HSI_DIR_CAND': ['Z/train/X', 'train/X', 'Train/X'],\n",
    "    'TRAIN_LR_HSI_DIR_CAND': ['Z/train/X_blur', 'train/X_blur', 'Train/X_blur'],\n",
    "    'TRAIN_HR_MSI_DIR_CAND': ['Z/train/Y', 'train/Y', 'Train/Y'],\n",
    "    'TEST_LR_HSI_DIR_CAND':  ['Z/test/X', 'test/X', 'Test/X'],\n",
    "    'TEST_HR_MSI_DIR_CAND':  ['Z/test/Y', 'test/Y', 'Test/Y'],\n",
    "    'TEST_GT_HR_HSI_DIR_CAND': ['Z/test/GT_HR', 'test/GT_HR', 'Test/GT_HR'],\n",
    "    'USE_GT': False,\n",
    "    'MSI_BANDS_SELECT': None,          # list[int] or None\n",
    "    'PATCH_HR_SIZE': 20,\n",
    "    'PATCH_STRIDE': 7,\n",
    "    'EDGE_OVERLAP': 2,\n",
    "    'MAX_TRAIN_SCENES': None,\n",
    "    'MAX_TEST_SCENES': None,\n",
    "    'EPOCHS': 50,\n",
    "    'BATCH_SIZE': 64,\n",
    "    'LEARNING_RATE': 1e-4,\n",
    "    'SAVE_MODEL_PATH': '/kaggle/working/BASFE_CAVE_init.keras',\n",
    "    'SAVE_MODEL_TRAINED_PATH': '/kaggle/working/BASFE_CAVE_trained.keras',\n",
    "    'LOG_DIR': '/kaggle/working/logs',\n",
    "    'RESULTS_DIR': '/kaggle/working/results',\n",
    "    'CHECKPOINT_DIR': '/kaggle/working/checkpoints',\n",
    "    'CSV_LOG': '/kaggle/working/training_log.csv',\n",
    "    'EARLY_STOP_PATIENCE': 10,\n",
    "    'SEED': 42,\n",
    "    # Memory / Patch Controls\n",
    "    'PATCH_MEMORY_CAP_MB': 4000,\n",
    "    'SUBSAMPLE_PATCH_RATE': 1,\n",
    "    'TRUNCATE_TO_CAP': True,\n",
    "    'MIXED_PRECISION': 'fp16',  # 'fp16','bf16',None\n",
    "    # Progress / Verbosity\n",
    "    'SHOW_CONFIG_SUMMARY': True,\n",
    "    'TRAIN_PROGRESS_EVERY_BATCHES': 25,\n",
    "    'PRED_PATCH_PROGRESS': True,\n",
    "    'SKIP_PLOT': False,          # Skip sample visualization for speed\n",
    "    # New Performance Knobs\n",
    "    'FAST_DEBUG_MODE': False,    # If True overrides some heavy params for quick smoke run\n",
    "    'RANDOM_PATCHES_PER_SCENE': None,  # If set (int), sample that many random patch positions per scene\n",
    "    'TOTAL_PATCHES_LIMIT': None, # Hard stop once this many patches collected (after subsampling)\n",
    "    'BUILD_PATCHES_MODE': 'grid',# 'grid' or 'random' (random = uniform random positions limited by RANDOM_PATCHES_PER_SCENE)\n",
    "    'ESTIMATE_ONLY': False,      # If True, only estimate patch counts & memory then stop (no training)\n",
    "}\n",
    "\n",
    "# FAST_DEBUG_MODE overrides\n",
    "if CONFIG['FAST_DEBUG_MODE']:\n",
    "    CONFIG['EPOCHS'] = min(CONFIG['EPOCHS'], 2)\n",
    "    CONFIG['MAX_TRAIN_SCENES'] = 1 if CONFIG['MAX_TRAIN_SCENES'] is None else min(CONFIG['MAX_TRAIN_SCENES'],1)\n",
    "    CONFIG['MAX_TEST_SCENES'] = 1 if CONFIG['MAX_TEST_SCENES'] is None else min(CONFIG['MAX_TEST_SCENES'],1)\n",
    "    CONFIG['SUBSAMPLE_PATCH_RATE'] = max(CONFIG['SUBSAMPLE_PATCH_RATE'], 4)\n",
    "    CONFIG['RANDOM_PATCHES_PER_SCENE'] = 200 if CONFIG['RANDOM_PATCHES_PER_SCENE'] is None else min(CONFIG['RANDOM_PATCHES_PER_SCENE'],200)\n",
    "    print('FAST_DEBUG_MODE active: parameters reduced for speed.')\n",
    "\n",
    "for d in ['LOG_DIR','RESULTS_DIR','CHECKPOINT_DIR']:\n",
    "    os.makedirs(CONFIG[d], exist_ok=True)\n",
    "\n",
    "# Optional: enable mixed precision\n",
    "if CONFIG['MIXED_PRECISION']:\n",
    "    try:\n",
    "        from tensorflow.keras import mixed_precision\n",
    "        policy = mixed_precision.Policy(CONFIG['MIXED_PRECISION'])\n",
    "        mixed_precision.set_global_policy(policy)\n",
    "        print('Mixed precision enabled with policy:', policy)\n",
    "    except Exception as e:\n",
    "        print('Mixed precision request failed, proceeding without:', e)\n",
    "\n",
    "random.seed(CONFIG['SEED'])\n",
    "np.random.seed(CONFIG['SEED'])\n",
    "\n",
    "def resolve_dir(candidates, required=True):\n",
    "    for rel in candidates:\n",
    "        full = os.path.join(CONFIG['ROOT_DIR'], rel)\n",
    "        if os.path.isdir(full):\n",
    "            return rel\n",
    "    if required:\n",
    "        raise FileNotFoundError(f\"None of these candidate paths exist under ROOT_DIR={CONFIG['ROOT_DIR']}: {candidates}\")\n",
    "    return None\n",
    "\n",
    "TRAIN_HR_HSI_DIR = resolve_dir(CONFIG['TRAIN_HR_HSI_DIR_CAND'])\n",
    "TRAIN_LR_HSI_DIR = resolve_dir(CONFIG['TRAIN_LR_HSI_DIR_CAND'])\n",
    "TRAIN_HR_MSI_DIR = resolve_dir(CONFIG['TRAIN_HR_MSI_DIR_CAND'])\n",
    "TEST_LR_HSI_DIR  = resolve_dir(CONFIG['TEST_LR_HSI_DIR_CAND'])\n",
    "TEST_HR_MSI_DIR  = resolve_dir(CONFIG['TEST_HR_MSI_DIR_CAND'])\n",
    "TEST_GT_HR_HSI_DIR = resolve_dir(CONFIG['TEST_GT_HR_HSI_DIR_CAND'], required=False) if CONFIG['USE_GT'] else None\n",
    "\n",
    "print('Resolved directories:')\n",
    "print('  TRAIN_HR_HSI_DIR ->', TRAIN_HR_HSI_DIR)\n",
    "print('  TRAIN_LR_HSI_DIR ->', TRAIN_LR_HSI_DIR)\n",
    "print('  TRAIN_HR_MSI_DIR ->', TRAIN_HR_MSI_DIR)\n",
    "print('  TEST_LR_HSI_DIR  ->', TEST_LR_HSI_DIR)\n",
    "print('  TEST_HR_MSI_DIR  ->', TEST_HR_MSI_DIR)\n",
    "print('  TEST_GT_HR_HSI_DIR ->', TEST_GT_HR_HSI_DIR)\n",
    "\n",
    "print('Host:', socket.gethostname())\n",
    "print('Python:', sys.version.split()[0], 'Platform:', platform.platform())\n",
    "print('GPU Available:', tf.config.list_physical_devices('GPU'))\n",
    "\n",
    "IGNORE_KEYS = {'__globals__', '__header__', '__version__'}\n",
    "\n",
    "def load_first_cube(mat_path):\n",
    "    mat = sio.loadmat(mat_path)\n",
    "    for k,v in mat.items():\n",
    "        if k in IGNORE_KEYS: continue\n",
    "        if isinstance(v, np.ndarray) and v.ndim == 3 and v.shape[2] >= 3:\n",
    "            arr = v.astype(np.float32)\n",
    "            vmin, vmax = arr.min(), arr.max()\n",
    "            if vmax > vmin:\n",
    "                arr = (arr - vmin)/(vmax - vmin)\n",
    "            return arr, k\n",
    "    raise ValueError(f'No 3D cube found in {mat_path}')\n",
    "\n",
    "def list_mats(rel_dir):\n",
    "    full = os.path.join(CONFIG['ROOT_DIR'], rel_dir)\n",
    "    if not os.path.isdir(full):\n",
    "        raise FileNotFoundError(f'Directory not found: {full}')\n",
    "    files = sorted([f for f in os.listdir(full) if f.lower().endswith('.mat')])\n",
    "    return [os.path.join(full, f) for f in files]\n",
    "\n",
    "train_hr_hsi_files = list_mats(TRAIN_HR_HSI_DIR)\n",
    "train_lr_hsi_files = list_mats(TRAIN_LR_HSI_DIR)\n",
    "train_hr_msi_files = list_mats(TRAIN_HR_MSI_DIR)\n",
    "\n",
    "if CONFIG['MAX_TRAIN_SCENES']:\n",
    "    train_hr_hsi_files = train_hr_hsi_files[:CONFIG['MAX_TRAIN_SCENES']]\n",
    "    train_lr_hsi_files = train_lr_hsi_files[:CONFIG['MAX_TRAIN_SCENES']]\n",
    "    train_hr_msi_files = train_hr_msi_files[:CONFIG['MAX_TRAIN_SCENES']]\n",
    "\n",
    "print('Train HR-HSI:', len(train_hr_hsi_files), 'Train LR-HSI:', len(train_lr_hsi_files), 'Train HR-MSI:', len(train_hr_msi_files))\n",
    "assert len(train_hr_hsi_files)==len(train_lr_hsi_files)==len(train_hr_msi_files), 'Mismatch in training file counts'\n",
    "\n",
    "hr_sample,_ = load_first_cube(train_hr_hsi_files[0])\n",
    "lr_sample,_ = load_first_cube(train_lr_hsi_files[0])\n",
    "scale_y = hr_sample.shape[0] / lr_sample.shape[0]\n",
    "scale_x = hr_sample.shape[1] / lr_sample.shape[1]\n",
    "assert abs(scale_x - round(scale_x))<1e-3 and abs(scale_y - round(scale_y))<1e-3, 'Scale must be integer'\n",
    "SCALE = int(round(scale_x))\n",
    "print('Inferred scale:', SCALE)\n",
    "\n",
    "msi_sample,_ = load_first_cube(train_hr_msi_files[0])\n",
    "H_BANDS = hr_sample.shape[2]\n",
    "M_BANDS = msi_sample.shape[2] if CONFIG['MSI_BANDS_SELECT'] is None else len(CONFIG['MSI_BANDS_SELECT'])\n",
    "print('Bands HR-HSI:', H_BANDS, 'Bands HR-MSI (used):', M_BANDS)\n",
    "\n",
    "hrsize = CONFIG['PATCH_HR_SIZE']\n",
    "stride = CONFIG['PATCH_STRIDE']\n",
    "num_filter = 32\n",
    "msi_bands = M_BANDS\n",
    "hsi_bands = H_BANDS\n",
    "scale = SCALE\n",
    "\n",
    "RESOLVED_PATHS = {\n",
    "    'TRAIN_HR_HSI_DIR': TRAIN_HR_HSI_DIR,\n",
    "    'TRAIN_LR_HSI_DIR': TRAIN_LR_HSI_DIR,\n",
    "    'TRAIN_HR_MSI_DIR': TRAIN_HR_MSI_DIR,\n",
    "    'TEST_LR_HSI_DIR': TEST_LR_HSI_DIR,\n",
    "    'TEST_HR_MSI_DIR': TEST_HR_MSI_DIR,\n",
    "    'TEST_GT_HR_HSI_DIR': TEST_GT_HR_HSI_DIR,\n",
    "}\n",
    "\n",
    "if CONFIG['SHOW_CONFIG_SUMMARY']:\n",
    "    print('\\n--- CONFIG SUMMARY ---')\n",
    "    show_keys = [\n",
    "        'ROOT_DIR','PATCH_HR_SIZE','PATCH_STRIDE','EDGE_OVERLAP','EPOCHS','BATCH_SIZE',\n",
    "        'LEARNING_RATE','PATCH_MEMORY_CAP_MB','SUBSAMPLE_PATCH_RATE','MIXED_PRECISION',\n",
    "        'RANDOM_PATCHES_PER_SCENE','TOTAL_PATCHES_LIMIT','BUILD_PATCHES_MODE','FAST_DEBUG_MODE'\n",
    "    ]\n",
    "    for k in show_keys:\n",
    "        print(f'{k}:', CONFIG[k])\n",
    "    print('----------------------\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oZzq3xoA7mpE"
   },
   "source": [
    "# Build Training Patch Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BSQxDx_gDBpI"
   },
   "outputs": [],
   "source": [
    "# Construct aligned patch tensors with performance modes (grid or random) and limits\n",
    "start_time = time.time()\n",
    "patches_hr = []\n",
    "patches_lr = []\n",
    "patches_mr = []\n",
    "scene_patch_counts = {}\n",
    "cap_bytes = CONFIG['PATCH_MEMORY_CAP_MB'] * 1_000_000\n",
    "subsample = max(1, CONFIG['SUBSAMPLE_PATCH_RATE'])\n",
    "\n",
    "est_patch_bytes = None\n",
    "MODE = CONFIG['BUILD_PATCHES_MODE']\n",
    "rand_per_scene = CONFIG['RANDOM_PATCHES_PER_SCENE']\n",
    "limit_total = CONFIG['TOTAL_PATCHES_LIMIT']\n",
    "\n",
    "scene_iter = tqdm(list(zip(train_hr_hsi_files, train_lr_hsi_files, train_hr_msi_files)), desc=f'Scenes (train) mode={MODE}', leave=True)\n",
    "for scene_index, (hr_path, lr_path, msi_path) in enumerate(scene_iter):\n",
    "    hr_cube,_ = load_first_cube(hr_path)\n",
    "    lr_cube,_ = load_first_cube(lr_path)\n",
    "    msi_cube,_ = load_first_cube(msi_path)\n",
    "    if CONFIG['MSI_BANDS_SELECT']:\n",
    "        msi_cube = msi_cube[:,:,CONFIG['MSI_BANDS_SELECT']]\n",
    "    up_lr = cv.resize(lr_cube, (hr_cube.shape[1], hr_cube.shape[0]), interpolation=cv.INTER_CUBIC)\n",
    "    h, w, _ = hr_cube.shape\n",
    "    c_before = len(patches_hr)\n",
    "\n",
    "    if MODE == 'grid':\n",
    "        row_starts = np.arange(0, h - hrsize, stride)\n",
    "        col_starts = np.arange(0, w - hrsize, stride)\n",
    "        total_grid = len(row_starts) * len(col_starts)\n",
    "        patch_iter = tqdm(total=total_grid, desc=f'Patches[{os.path.basename(hr_path)}]', leave=False)\n",
    "        local_added = 0\n",
    "        stop_scene = False\n",
    "        for i in row_starts:\n",
    "            for j in col_starts:\n",
    "                if (local_added % subsample) == 0:\n",
    "                    patches_hr.append(hr_cube[i:i+hrsize, j:j+hrsize, :])\n",
    "                    patches_lr.append(up_lr[i:i+hrsize, j:j+hrsize, :])\n",
    "                    patches_mr.append(msi_cube[i:i+hrsize, j:j+hrsize, :])\n",
    "                    if est_patch_bytes is None:\n",
    "                        est_patch_bytes = (patches_hr[-1].nbytes + patches_lr[-1].nbytes + patches_mr[-1].nbytes)\n",
    "                    total_bytes = len(patches_hr) * est_patch_bytes\n",
    "                    if cap_bytes and total_bytes > cap_bytes:\n",
    "                        print(f\"Memory cap reached (~{total_bytes/1e6:.1f} MB). Stopping patch collection.\")\n",
    "                        stop_scene = True\n",
    "                        break\n",
    "                local_added += 1\n",
    "                patch_iter.update(1)\n",
    "                if limit_total and len(patches_hr) >= limit_total:\n",
    "                    stop_scene = True\n",
    "                    break\n",
    "            if stop_scene:\n",
    "                break\n",
    "        patch_iter.close()\n",
    "    else:  # random mode\n",
    "        # Determine number of valid upper-left coordinates\n",
    "        max_i = h - hrsize\n",
    "        max_j = w - hrsize\n",
    "        n_samples = rand_per_scene if rand_per_scene else 0\n",
    "        if n_samples == 0:\n",
    "            # estimate comparable to grid count but reduced by stride\n",
    "            approx_rows = max(1, (h - hrsize)//stride)\n",
    "            approx_cols = max(1, (w - hrsize)//stride)\n",
    "            n_samples = int(approx_rows * approx_cols)\n",
    "        patch_iter = tqdm(range(n_samples), desc=f'RandPatches[{os.path.basename(hr_path)}]', leave=False)\n",
    "        for k in patch_iter:\n",
    "            i = random.randint(0, max_i)\n",
    "            j = random.randint(0, max_j)\n",
    "            if (k % subsample) == 0:\n",
    "                patches_hr.append(hr_cube[i:i+hrsize, j:j+hrsize, :])\n",
    "                patches_lr.append(up_lr[i:i+hrsize, j:j+hrsize, :])\n",
    "                patches_mr.append(msi_cube[i:i+hrsize, j:j+hrsize, :])\n",
    "                if est_patch_bytes is None:\n",
    "                    est_patch_bytes = (patches_hr[-1].nbytes + patches_lr[-1].nbytes + patches_mr[-1].nbytes)\n",
    "                total_bytes = len(patches_hr) * est_patch_bytes\n",
    "                if cap_bytes and total_bytes > cap_bytes:\n",
    "                    print(f\"Memory cap reached (~{total_bytes/1e6:.1f} MB). Stopping patch collection.\")\n",
    "                    break\n",
    "                if limit_total and len(patches_hr) >= limit_total:\n",
    "                    break\n",
    "        patch_iter.close()\n",
    "    scene_patch_counts[os.path.basename(hr_path)] = len(patches_hr) - c_before\n",
    "    if (cap_bytes and est_patch_bytes and (len(patches_hr) * est_patch_bytes) > cap_bytes) or (limit_total and len(patches_hr) >= limit_total):\n",
    "        print('Global stop condition reached (memory or total patches).')\n",
    "        break\n",
    "\n",
    "if len(patches_hr) == 0:\n",
    "    raise RuntimeError('No patches collected; adjust configuration (patch size/stride/mode).')\n",
    "\n",
    "if CONFIG['ESTIMATE_ONLY']:\n",
    "    est_total_mb = (len(patches_hr) * est_patch_bytes)/1e6 if est_patch_bytes else 0\n",
    "    print(f'[ESTIMATE_ONLY] Collected {len(patches_hr)} patches (~{est_total_mb:.1f} MB for all three tensors). Stopping before stacking.')\n",
    "else:\n",
    "    hrdata = np.stack(patches_hr, axis=0)\n",
    "    lrdata = np.stack(patches_lr, axis=0)\n",
    "    mrdata = np.stack(patches_mr, axis=0)\n",
    "\n",
    "    if CONFIG['TRUNCATE_TO_CAP'] and est_patch_bytes and (hrdata.nbytes + lrdata.nbytes + mrdata.nbytes) > cap_bytes:\n",
    "        triplet_bytes = (hrdata[0].nbytes + lrdata[0].nbytes + mrdata[0].nbytes)\n",
    "        max_samples = int(cap_bytes // triplet_bytes)\n",
    "        if max_samples < hrdata.shape[0]:\n",
    "            idx = np.random.permutation(hrdata.shape[0])[:max_samples]\n",
    "            hrdata = hrdata[idx]; lrdata = lrdata[idx]; mrdata = mrdata[idx]\n",
    "            print(f'Truncated to {max_samples} samples to meet memory cap.')\n",
    "\n",
    "    elapsed = time.time()-start_time\n",
    "    print(f'Patch extraction finished in {elapsed:.1f}s')\n",
    "    print('Training patches shapes -> hrdata:', hrdata.shape, 'lrdata:', lrdata.shape, 'mrdata:', mrdata.shape)\n",
    "    print('Approx total patch tensors memory MB:', (hrdata.nbytes+lrdata.nbytes+mrdata.nbytes)/1e6)\n",
    "    print('Patches per scene (first 5):', list(scene_patch_counts.items())[:5])\n",
    "\n",
    "    if not CONFIG['SKIP_PLOT']:\n",
    "        if hrdata.shape[0] > 10:\n",
    "            n = min(10, hrdata.shape[0]-1)\n",
    "        else:\n",
    "            n = 0\n",
    "        i = hrdata.shape[3]//2\n",
    "        p = np.hstack((hrdata[n,:,:,i], lrdata[n,:,:,i], mrdata[n,:,:,0]))\n",
    "        fig, ax = plt.subplots(figsize=(6,6))\n",
    "        pyplot.imshow(p, vmin=0, vmax=1, cmap='gray')\n",
    "        pyplot.title('HR | UpLR | MSI(first band)')\n",
    "        pyplot.axis('off')\n",
    "        plt.show()\n",
    "    print('Memory usage (approx) HR patches MB:', hrdata.nbytes/1e6 if not CONFIG['ESTIMATE_ONLY'] else 'N/A (estimate only)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XdiGN0_qs52k"
   },
   "outputs": [],
   "source": [
    "# Placeholder cell retained (was sample visualization) -- already visualized above.\n",
    "print('Sample mid training patch stats:')\n",
    "print('HR mean', hrdata.mean(), 'LR mean', lrdata.mean(), 'MR mean', mrdata.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J2hRM89f7y5q"
   },
   "source": [
    "# Model (Unmodified BASFE Architecture)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YzUMjoVTvWNG"
   },
   "outputs": [],
   "source": [
    "# Model definition using explicit PReLU layers (fix for activation='PReLU' ValueError)\n",
    "# Architectural logic (number of convolutions, filter sizes, skip connections, concat order) kept identical.\n",
    "num_filter = 32\n",
    "\n",
    "# Helper blocks replicating original spec & spat residual patterns\n",
    "# spec: two 1x1 convs with PReLU then residual sum\n",
    "# spat: two 3x3 convs with PReLU then residual sum\n",
    "def spec(inputs, nf):\n",
    "    x = layers.Conv2D(nf, 1, padding=\"same\", use_bias=True)(inputs)\n",
    "    x = layers.PReLU(shared_axes=[1,2])(x)\n",
    "    x = layers.Conv2D(nf, 1, padding=\"same\", use_bias=True)(x)\n",
    "    x = layers.PReLU(shared_axes=[1,2])(x)\n",
    "    return layers.Add()([inputs, x])\n",
    "\n",
    "def spat(inputs, nf):\n",
    "    x = layers.Conv2D(nf, 3, padding=\"same\", use_bias=True)(inputs)\n",
    "    x = layers.PReLU(shared_axes=[1,2])(x)\n",
    "    x = layers.Conv2D(nf, 3, padding=\"same\", use_bias=True)(x)\n",
    "    x = layers.PReLU(shared_axes=[1,2])(x)\n",
    "    return layers.Add()([inputs, x])\n",
    "\n",
    "# ****************************** MSI Encoder ******************************\n",
    "msi_input = keras.Input(shape=(hrsize, hrsize, msi_bands), name=\"msi_input\")\n",
    "x01 = layers.Conv2D(num_filter, 3, padding=\"same\", use_bias=True)(msi_input)\n",
    "x01 = layers.PReLU(shared_axes=[1,2])(x01)\n",
    "x02 = spec(x01,num_filter)\n",
    "x02 = spat(x02,num_filter)\n",
    "x03 = layers.Concatenate()([x01, x02])\n",
    "\n",
    "x04 = layers.Conv2D(num_filter, 3, padding=\"same\", use_bias=True)(x03)\n",
    "x04 = layers.PReLU(shared_axes=[1,2])(x04)\n",
    "x05 = spec(x04,num_filter)\n",
    "x05 = spat(x05,num_filter)\n",
    "x06 = layers.Concatenate()([x01, x04, x05])\n",
    "\n",
    "x07 = layers.Conv2D(num_filter, 5, padding=\"same\", use_bias=True)(x06)\n",
    "x07 = layers.PReLU(shared_axes=[1,2])(x07)\n",
    "x07 = spec(x07,num_filter)\n",
    "x08 = spat(x07,num_filter)\n",
    "\n",
    "# ****************************** LR_HSI Encoder ******************************\n",
    "lr_input = keras.Input(shape=(hrsize, hrsize,hsi_bands), name=\"lr_input\")\n",
    "x11 = layers.Conv2D(num_filter, 3, padding=\"same\", use_bias=True)(lr_input)\n",
    "x11 = layers.PReLU(shared_axes=[1,2])(x11)\n",
    "x12 = spec(x11,num_filter)\n",
    "x12 = spat(x12,num_filter)\n",
    "x13 = layers.Concatenate()([x11, x12])\n",
    "\n",
    "x14 = layers.Conv2D(num_filter, 3, padding=\"same\", use_bias=True)(x13)\n",
    "x14 = layers.PReLU(shared_axes=[1,2])(x14)\n",
    "x15 = spec(x14,num_filter)\n",
    "x15 = spat(x15,num_filter)\n",
    "x16 = layers.Concatenate()([x11, x14, x15])\n",
    "\n",
    "x17 = layers.Conv2D(num_filter, 5, padding=\"same\", use_bias=True)(x16)\n",
    "x17 = layers.PReLU(shared_axes=[1,2])(x17)\n",
    "x17 = spec(x17,num_filter)\n",
    "x18 = spat(x17,num_filter)\n",
    "\n",
    "# ********************************* concat ***********************************\n",
    "x21 = layers.Concatenate()([x01, x04, x07, x08, x11, x14, x17, x18])\n",
    "\n",
    "x22 = layers.Conv2D(hsi_bands, 3, padding=\"same\", use_bias=True)(x21)\n",
    "x22 = layers.PReLU(shared_axes=[1,2])(x22)\n",
    "fuse_output = layers.Conv2D(hsi_bands, 3, padding=\"same\", use_bias=True, name=\"fuse_output\")(x22)\n",
    "fuse_output = layers.PReLU(shared_axes=[1,2], name='fuse_output_prelu')(fuse_output)\n",
    "\n",
    "# IMPORTANT: provide a single tensor (not list) for outputs to avoid target structure ambiguity in Keras 3 / optree.\n",
    "model = keras.Model(\n",
    "    inputs=[msi_input, lr_input],\n",
    "    outputs=fuse_output,\n",
    "    name='BASFE_Fusion'\n",
    ")\n",
    "model.summary()\n",
    "# plot_model may fail in headless or restricted envs; wrap in try/except\n",
    "try:\n",
    "    keras.utils.plot_model(model, \"multi_input_and_output_model.png\", show_shapes=True)\n",
    "except Exception as e:\n",
    "    print('plot_model skipped:', e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pfk9z5gW74Gy"
   },
   "source": [
    "# Training BASFE on CAVE-style Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kF49KV5v25Gl"
   },
   "outputs": [],
   "source": [
    "# Save initial (untrained) model with defensive wrapper\n",
    "try:\n",
    "    model.save(CONFIG['SAVE_MODEL_PATH'])\n",
    "    print('Initial model saved to', CONFIG['SAVE_MODEL_PATH'])\n",
    "except Exception as e:\n",
    "    print('WARNING: initial model save failed:', e)\n",
    "\n",
    "# Custom progress callback\n",
    "class BatchProgress(keras.callbacks.Callback):\n",
    "    def __init__(self, every_n=50, total_batches=None):\n",
    "        super().__init__()\n",
    "        self.every_n = every_n\n",
    "        self.total_batches = total_batches\n",
    "        self.start_time = None\n",
    "    def on_train_begin(self, logs=None):\n",
    "        self.start_time = time.time()\n",
    "    def on_batch_end(self, batch, logs=None):\n",
    "        if (batch+1) % self.every_n == 0:\n",
    "            elapsed = time.time() - self.start_time\n",
    "            if self.total_batches:\n",
    "                rate = (batch+1)/elapsed\n",
    "                remaining_batches = self.total_batches - (batch+1)\n",
    "                eta = remaining_batches / max(rate,1e-6)\n",
    "                print(f\"Batch {batch+1}/{self.total_batches} - loss {logs.get('loss'):.5f} - elapsed {elapsed:.1f}s - ETA {eta:.1f}s\")\n",
    "            else:\n",
    "                print(f\"Batch {batch+1} - loss {logs.get('loss'):.5f} - elapsed {elapsed:.1f}s\")\n",
    "\n",
    "# Estimate total batches for progress messages\n",
    "steps_per_epoch = math.ceil(hrdata.shape[0] / CONFIG['BATCH_SIZE'])\n",
    "\n",
    "# Callbacks for richer logging\n",
    "callbacks = []\n",
    "callbacks.append(keras.callbacks.CSVLogger(CONFIG['CSV_LOG'], append=False))\n",
    "callbacks.append(keras.callbacks.ModelCheckpoint(\n",
    "    filepath=os.path.join(CONFIG['CHECKPOINT_DIR'], 'epoch_{epoch:03d}_loss_{loss:.5f}.keras'),\n",
    "    monitor='loss', save_best_only=True, save_weights_only=False, verbose=1))\n",
    "callbacks.append(keras.callbacks.EarlyStopping(monitor='loss', patience=CONFIG['EARLY_STOP_PATIENCE'], restore_best_weights=True, verbose=1))\n",
    "callbacks.append(keras.callbacks.TensorBoard(log_dir=CONFIG['LOG_DIR'], write_graph=False, update_freq='epoch'))\n",
    "callbacks.append(BatchProgress(every_n=CONFIG['TRAIN_PROGRESS_EVERY_BATCHES'], total_batches=steps_per_epoch))\n",
    "\n",
    "model.compile(\n",
    "    optimizer=tf.optimizers.Adam(learning_rate=CONFIG['LEARNING_RATE']),\n",
    "    loss=keras.losses.MeanSquaredError(),\n",
    ")\n",
    "\n",
    "print('Starting training: epochs', CONFIG['EPOCHS'], 'batch size', CONFIG['BATCH_SIZE'], 'steps/epoch', steps_per_epoch)\n",
    "start_train = time.time()\n",
    "\n",
    "history = model.fit(\n",
    "    {\"msi_input\": mrdata, \"lr_input\": lrdata},\n",
    "    hrdata,\n",
    "    epochs=CONFIG['EPOCHS'],\n",
    "    batch_size=CONFIG['BATCH_SIZE'],\n",
    "    shuffle=True,\n",
    "    verbose=1,\n",
    "    callbacks=callbacks,\n",
    ")\n",
    "train_time = time.time()-start_train\n",
    "print(f'Training completed in {train_time/60:.2f} min')\n",
    "\n",
    "try:\n",
    "    model.save(CONFIG['SAVE_MODEL_TRAINED_PATH'])\n",
    "    print('Trained model saved to', CONFIG['SAVE_MODEL_TRAINED_PATH'])\n",
    "except Exception as e:\n",
    "    print('WARNING: trained model save failed:', e)\n",
    "\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.plot(history.history['loss'], label='loss')\n",
    "plt.xlabel('Epoch'); plt.ylabel('MSE Loss'); plt.title('Training Loss Curve'); plt.grid(True); plt.legend();\n",
    "plt.tight_layout(); plt.show()\n",
    "\n",
    "# Summary JSON\n",
    "summary_info = {\n",
    "    'epochs_ran': len(history.history['loss']),\n",
    "    'final_loss': float(history.history['loss'][-1]),\n",
    "    'train_time_sec': train_time,\n",
    "    'num_patches': int(hrdata.shape[0]),\n",
    "    'scale': scale,\n",
    "    'hr_patch_size': hrsize,\n",
    "    'bands_hsi': hsi_bands,\n",
    "    'bands_msi': msi_bands,\n",
    "    'mixed_precision': CONFIG['MIXED_PRECISION'],\n",
    "    'patch_memory_cap_mb': CONFIG['PATCH_MEMORY_CAP_MB'],\n",
    "    'subsample_rate': CONFIG['SUBSAMPLE_PATCH_RATE'],\n",
    "}\n",
    "with open(os.path.join(CONFIG['RESULTS_DIR'], 'training_summary.json'), 'w') as f:\n",
    "    json.dump(summary_info, f, indent=2)\n",
    "print('Saved training summary to training_summary.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A7eycocrLFxx"
   },
   "source": [
    "# Reconstruction & Assessment on Test Scenes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0Gnydw_G8B-2"
   },
   "outputs": [],
   "source": [
    "# Prepare test inputs and run model prediction over each scene via tiling reconstruction with progress indicators.\n",
    "\n",
    "def prepare_test_scene(lr_path, msi_path):\n",
    "    lr_cube,_ = load_first_cube(lr_path)\n",
    "    msi_cube,_ = load_first_cube(msi_path)\n",
    "    if CONFIG['MSI_BANDS_SELECT']:\n",
    "        msi_cube = msi_cube[:,:,CONFIG['MSI_BANDS_SELECT']]\n",
    "    up_lr = cv.resize(lr_cube, (msi_cube.shape[1], msi_cube.shape[0]), interpolation=cv.INTER_CUBIC)\n",
    "    return up_lr, msi_cube\n",
    "\n",
    "TEST_LR = list_mats(RESOLVED_PATHS['TEST_LR_HSI_DIR'])\n",
    "TEST_MSI = list_mats(RESOLVED_PATHS['TEST_HR_MSI_DIR'])\n",
    "if CONFIG['MAX_TEST_SCENES']:\n",
    "    TEST_LR = TEST_LR[:CONFIG['MAX_TEST_SCENES']]\n",
    "    TEST_MSI = TEST_MSI[:CONFIG['MAX_TEST_SCENES']]\n",
    "assert len(TEST_LR)==len(TEST_MSI), 'Mismatch in test file counts'\n",
    "\n",
    "EDGE = CONFIG['EDGE_OVERLAP']\n",
    "strider = hrsize - 2*EDGE\n",
    "\n",
    "reconstructed = {}\n",
    "\n",
    "# Batch size for prediction to limit memory in case of many patches\n",
    "PRED_BATCH = max(1, 2048 // (hrsize*hrsize))  # heuristic; adjust if needed\n",
    "print('Prediction batch heuristic:', PRED_BATCH)\n",
    "\n",
    "for lr_path, msi_path in tqdm(list(zip(TEST_LR, TEST_MSI)), desc='Scenes (test)', leave=True):\n",
    "    scene_name = os.path.splitext(os.path.basename(lr_path))[0]\n",
    "    up_lr, hr_msi = prepare_test_scene(lr_path, msi_path)\n",
    "    H, W, _ = hr_msi.shape\n",
    "    ii = np.arange(0, H, strider)\n",
    "    jj = np.arange(0, W, strider)\n",
    "    if ii[-1] + hrsize > H: ii[-1] = H - hrsize\n",
    "    if len(ii) > 1 and ii[-2] >= ii[-1]: ii = ii[:-1]\n",
    "    if jj[-1] + hrsize > W: jj[-1] = W - hrsize\n",
    "    if len(jj) > 1 and jj[-2] >= jj[-1]: jj = jj[:-1]\n",
    "\n",
    "    mr_patches = []\n",
    "    lr_patches = []\n",
    "    grid_total = len(ii)*len(jj)\n",
    "    for i in ii:\n",
    "        for j in jj:\n",
    "            mr_patches.append(hr_msi[i:i+hrsize, j:j+hrsize, :])\n",
    "            lr_patches.append(up_lr[i:i+hrsize, j:j+hrsize, :])\n",
    "    mrdatainput = np.stack(mr_patches, axis=0)\n",
    "    lrdatainput = np.stack(lr_patches, axis=0)\n",
    "\n",
    "    preds_list = []\n",
    "    batch_iter = tqdm(range(0, mrdatainput.shape[0], PRED_BATCH), desc=f'Predict[{scene_name}]', leave=False)\n",
    "    for start in batch_iter:\n",
    "        end = start + PRED_BATCH\n",
    "        batch_preds = model.predict((mrdatainput[start:end], lrdatainput[start:end]), verbose=0)\n",
    "        preds_list.append(batch_preds)\n",
    "    preds = np.concatenate(preds_list, axis=0)\n",
    "\n",
    "    out_cube = np.zeros((H, W, hsi_bands), dtype=np.float32)\n",
    "    count = 0\n",
    "    for i in ii:\n",
    "        for j in jj:\n",
    "            out_cube[i:i+hrsize, j:j+hrsize, :] = preds[count]\n",
    "            count += 1\n",
    "    count = 0\n",
    "    for i in ii:\n",
    "        for j in jj:\n",
    "            out_cube[i+EDGE:i+hrsize-EDGE, j+EDGE:j+hrsize-EDGE, :] = preds[count, EDGE:-EDGE, EDGE:-EDGE, :]\n",
    "            count += 1\n",
    "\n",
    "    reconstructed[scene_name] = out_cube\n",
    "    savemat(os.path.join(CONFIG['RESULTS_DIR'], f'{scene_name}_reconst.mat'), {f'reconst_{scene_name}': out_cube})\n",
    "    print(f'Reconstructed {scene_name}: shape {out_cube.shape}')\n",
    "\n",
    "print('Total test scenes reconstructed:', len(reconstructed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0RbcjOqCnyda"
   },
   "outputs": [],
   "source": [
    "# Metrics (per-scene if GT available) + consolidated logging\n",
    "from math import log10\n",
    "\n",
    "metrics_results = {}\n",
    "\n",
    "if not reconstructed:\n",
    "    print('No reconstructed scenes found; skipping metrics.')\n",
    "else:\n",
    "    if CONFIG['TEST_GT_HR_HSI_DIR']:\n",
    "        gt_files = list_mats(CONFIG['TEST_GT_HR_HSI_DIR'])\n",
    "        gt_map = {os.path.splitext(os.path.basename(f))[0]: f for f in gt_files}\n",
    "\n",
    "        def compute_metrics(pred, gt, scale):\n",
    "            assert pred.shape == gt.shape\n",
    "            z = pred.shape\n",
    "            n = z[0]*z[1]\n",
    "            L = z[2]\n",
    "            temp = np.sum(np.sum((pred-gt)*(pred-gt),axis=0),axis=0)/n\n",
    "            rmse_per_band = np.sqrt(temp)\n",
    "            rmse_total = np.sqrt(np.sum(temp)/L)\n",
    "            psnr = 10*log10(1.0/(rmse_total**2 + 1e-12))\n",
    "            num = np.sum(pred*gt,axis=2)\n",
    "            den = np.sqrt(np.sum(pred*pred,axis=2)*np.sum(gt*gt,axis=2))+1e-12\n",
    "            sam = np.mean(np.arccos(np.clip(num/den, -1, 1))) * 180/np.pi\n",
    "            mean_gt = np.sum(np.sum(gt,axis=0),axis=0)/n\n",
    "            ergas = 100/scale*np.sqrt(np.sum((rmse_per_band / (mean_gt+1e-12))**2)/L)\n",
    "            c1=.0001; c2=.0001\n",
    "            ssim = []\n",
    "            mean_p = np.sum(np.sum(pred,axis=0),axis=0)/n\n",
    "            for i in range(L):\n",
    "                sigma2_p = np.mean(pred[:,:,i]**2)-mean_p[i]**2\n",
    "                sigma2_g = np.mean(gt[:,:,i]**2)-mean_gt[i]**2\n",
    "                sigma_pg = np.mean(pred[:,:,i]*gt[:,:,i]) - mean_p[i]*mean_gt[i]\n",
    "                ssim_i = ((2*mean_p[i]*mean_gt[i]+c1)*(2*sigma_pg+c2))/((mean_p[i]**2+mean_gt[i]**2+c1)*(sigma2_p+sigma2_g+c2))\n",
    "                ssim.append(ssim_i)\n",
    "            mssim = float(np.mean(ssim))\n",
    "            cc = []\n",
    "            for i in range(L):\n",
    "                cc_num = np.sum((pred[:,:,i]-mean_p[i])*(gt[:,:,i]-mean_gt[i]))\n",
    "                cc_den = np.sqrt(np.sum((pred[:,:,i]-mean_p[i])**2)*np.sum((gt[:,:,i]-mean_gt[i])**2))+1e-12\n",
    "                cc.append(cc_num/cc_den)\n",
    "            return {\n",
    "                'RMSE': float(rmse_total),\n",
    "                'PSNR': float(psnr),\n",
    "                'SAM_deg': float(sam),\n",
    "                'ERGAS': float(ergas),\n",
    "                'MSSIM': float(np.mean(ssim)),\n",
    "                'CC': float(np.mean(cc)),\n",
    "            }\n",
    "\n",
    "        for scene in tqdm(reconstructed.keys(), desc='Metrics scenes', leave=True):\n",
    "            if scene in gt_map:\n",
    "                try:\n",
    "                    gt_cube,_ = load_first_cube(gt_map[scene])\n",
    "                    H = min(gt_cube.shape[0], reconstructed[scene].shape[0])\n",
    "                    W = min(gt_cube.shape[1], reconstructed[scene].shape[1])\n",
    "                    gt_crop = gt_cube[:H,:W,:reconstructed[scene].shape[2]]\n",
    "                    pred_crop = reconstructed[scene][:H,:W,:reconstructed[scene].shape[2]]\n",
    "                    metrics_results[scene] = compute_metrics(pred_crop, gt_crop, scale)\n",
    "                except Exception as e:\n",
    "                    print(f'Failed metrics for {scene}:', e)\n",
    "        for s, m in metrics_results.items():\n",
    "            print('Metrics', s, m)\n",
    "    else:\n",
    "        print('No GT directory configured for test metrics; skipping metric computation.')\n",
    "\n",
    "# Save metrics JSON if any\n",
    "os.makedirs(CONFIG['RESULTS_DIR'], exist_ok=True)\n",
    "if metrics_results:\n",
    "    with open(os.path.join(CONFIG['RESULTS_DIR'], 'metrics.json'), 'w') as f:\n",
    "        json.dump(metrics_results, f, indent=2)\n",
    "    print('Saved metrics.json')\n",
    "print('Done.')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
