{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HChAL6ob6UZh"
   },
   "source": [
    "# CAVE / Generic Hyperspectral-Multispectral Fusion Notebook\n",
    "\n",
    "This notebook adapts the BASFE model (model cell left untouched) to work on a dataset laid out like:\n",
    "\n",
    "```\n",
    "root/\n",
    "  test/\n",
    "    X/  (LR-HSI *.mat)\n",
    "    Y/  (HR-MSI *.mat)\n",
    "  Z/\n",
    "    train/\n",
    "      X/       (HR-HSI ground-truth)\n",
    "      X_blur/  (LR-HSI versions matched to X)\n",
    "      Y/       (HR-MSI)\n",
    "```\n",
    "\n",
    "You may change the directory names in the configuration cell. The code will:\n",
    "- Load training scenes (HR-HSI, LR-HSI, HR-MSI)\n",
    "- Extract aligned patches\n",
    "- Train the BASFE model\n",
    "- Run inference on test scenes (LR-HSI + HR-MSI) and (optionally) compute metrics if HR-HSI GT is available\n",
    "\n",
    "Assumptions (can be adjusted in config cell):\n",
    "- Each `.mat` file contains at least one 3D array (H, W, Bands). The first suitable array is selected automatically.\n",
    "- Value ranges are normalized to [0,1].\n",
    "- Scale factor is inferred from HR vs LR spatial dimensions in training set (integer ratio).\n",
    "\n",
    "Modify `CONFIG` dict below to fit your filesystem.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WGwEd5Oi7SwD"
   },
   "source": [
    "# Configuration & Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7ZWv58ER5hzZ"
   },
   "outputs": [],
   "source": [
    "# If running in Colab uncomment the following two lines.\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')\n",
    "\n",
    "import os, sys, math, random, json, glob, time, socket, platform\n",
    "import numpy as np\n",
    "import cv2 as cv\n",
    "import scipy.io as sio\n",
    "from scipy.io import savemat\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import pyplot\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras import layers\n",
    "from tqdm import tqdm\n",
    "\n",
    "print('TensorFlow version:', tf.__version__)\n",
    "\n",
    "# ---------------- Configuration ----------------\n",
    "# For Kaggle, set ROOT_DIR to the dataset mount path, e.g. '/kaggle/input/cave-dataset'\n",
    "# Ensure the folder structure under ROOT_DIR matches:\n",
    "#   Z/train/X        (HR-HSI)\n",
    "#   Z/train/X_blur   (LR-HSI)\n",
    "#   Z/train/Y        (HR-MSI)\n",
    "#   test/X           (LR-HSI test)\n",
    "#   test/Y           (HR-MSI test)\n",
    "# Optionally: a GT HR-HSI test folder you can point TEST_GT_HR_HSI_DIR to.\n",
    "CONFIG = {\n",
    "    'ROOT_DIR': '/kaggle/input/cave-dataset',   # CHANGE IF DIFFERENT\n",
    "    'TRAIN_HR_HSI_DIR': 'Z/train/X',        # HR-HSI ground-truth\n",
    "    'TRAIN_LR_HSI_DIR': 'Z/train/X_blur',   # LR-HSI (blurred / downsampled)\n",
    "    'TRAIN_HR_MSI_DIR': 'Z/train/Y',        # HR-MSI\n",
    "    'TEST_LR_HSI_DIR': 'test/X',            # LR-HSI test\n",
    "    'TEST_HR_MSI_DIR': 'test/Y',            # HR-MSI test\n",
    "    'TEST_GT_HR_HSI_DIR': None,             # e.g. 'test/GT_HR' if available\n",
    "    'MSI_BANDS_SELECT': None,               # list[int] e.g. [10,15,20,25] or None\n",
    "    'PATCH_HR_SIZE': 20,\n",
    "    'PATCH_STRIDE': 7,\n",
    "    'EDGE_OVERLAP': 2,\n",
    "    'MAX_TRAIN_SCENES': None,               # reduce for fast debug\n",
    "    'MAX_TEST_SCENES': None,\n",
    "    'EPOCHS': 50,\n",
    "    'BATCH_SIZE': 64,\n",
    "    'LEARNING_RATE': 1e-4,\n",
    "    'SAVE_MODEL_PATH': '/kaggle/working/BASFE_CAVE_init',\n",
    "    'SAVE_MODEL_TRAINED_PATH': '/kaggle/working/BASFE_CAVE_trained',\n",
    "    'LOG_DIR': '/kaggle/working/logs',\n",
    "    'RESULTS_DIR': '/kaggle/working/results',\n",
    "    'CHECKPOINT_DIR': '/kaggle/working/checkpoints',\n",
    "    'CSV_LOG': '/kaggle/working/training_log.csv',\n",
    "    'EARLY_STOP_PATIENCE': 10,\n",
    "    'SEED': 42,\n",
    "}\n",
    "\n",
    "for d in ['LOG_DIR','RESULTS_DIR','CHECKPOINT_DIR']:\n",
    "    os.makedirs(CONFIG[d], exist_ok=True)\n",
    "\n",
    "random.seed(CONFIG['SEED'])\n",
    "np.random.seed(CONFIG['SEED'])\n",
    "\n",
    "# Quick environment summary\n",
    "print('Host:', socket.gethostname())\n",
    "print('Python:', sys.version.split()[0], 'Platform:', platform.platform())\n",
    "print('GPU Available:', tf.config.list_physical_devices('GPU'))\n",
    "\n",
    "# Utility: find first 3D array in .mat file\n",
    "IGNORE_KEYS = {'__globals__', '__header__', '__version__'}\n",
    "\n",
    "def load_first_cube(mat_path):\n",
    "    mat = sio.loadmat(mat_path)\n",
    "    for k,v in mat.items():\n",
    "        if k in IGNORE_KEYS: continue\n",
    "        if isinstance(v, np.ndarray) and v.ndim == 3 and v.shape[2] >= 3:\n",
    "            arr = v.astype(np.float32)\n",
    "            vmin, vmax = arr.min(), arr.max()\n",
    "            if vmax > vmin:\n",
    "                arr = (arr - vmin)/(vmax - vmin)\n",
    "            return arr, k\n",
    "    raise ValueError(f'No 3D cube found in {mat_path}')\n",
    "\n",
    "# List .mat files utility\n",
    "\n",
    "def list_mats(dir_path):\n",
    "    full = os.path.join(CONFIG['ROOT_DIR'], dir_path)\n",
    "    if not os.path.isdir(full):\n",
    "        raise FileNotFoundError(f'Directory not found: {full}')\n",
    "    files = sorted([f for f in os.listdir(full) if f.lower().endswith('.mat')])\n",
    "    return [os.path.join(full, f) for f in files]\n",
    "\n",
    "train_hr_hsi_files = list_mats(CONFIG['TRAIN_HR_HSI_DIR'])\n",
    "train_lr_hsi_files = list_mats(CONFIG['TRAIN_LR_HSI_DIR'])\n",
    "train_hr_msi_files = list_mats(CONFIG['TRAIN_HR_MSI_DIR'])\n",
    "\n",
    "if CONFIG['MAX_TRAIN_SCENES']:\n",
    "    train_hr_hsi_files = train_hr_hsi_files[:CONFIG['MAX_TRAIN_SCENES']]\n",
    "    train_lr_hsi_files = train_lr_hsi_files[:CONFIG['MAX_TRAIN_SCENES']]\n",
    "    train_hr_msi_files = train_hr_msi_files[:CONFIG['MAX_TRAIN_SCENES']]\n",
    "\n",
    "print('Train HR-HSI:', len(train_hr_hsi_files), 'Train LR-HSI:', len(train_lr_hsi_files), 'Train HR-MSI:', len(train_hr_msi_files))\n",
    "assert len(train_hr_hsi_files)==len(train_lr_hsi_files)==len(train_hr_msi_files), 'Mismatch in training file counts'\n",
    "\n",
    "# Infer scale from first pair\n",
    "hr_sample,_ = load_first_cube(train_hr_hsi_files[0])\n",
    "lr_sample,_ = load_first_cube(train_lr_hsi_files[0])\n",
    "scale_y = hr_sample.shape[0] / lr_sample.shape[0]\n",
    "scale_x = hr_sample.shape[1] / lr_sample.shape[1]\n",
    "assert abs(scale_x - round(scale_x))<1e-3 and abs(scale_y - round(scale_y))<1e-3, 'Scale must be integer'\n",
    "SCALE = int(round(scale_x))\n",
    "print('Inferred scale:', SCALE)\n",
    "\n",
    "msi_sample,_ = load_first_cube(train_hr_msi_files[0])\n",
    "H_BANDS = hr_sample.shape[2]\n",
    "M_BANDS = msi_sample.shape[2] if CONFIG['MSI_BANDS_SELECT'] is None else len(CONFIG['MSI_BANDS_SELECT'])\n",
    "print('Bands HR-HSI:', H_BANDS, 'Bands HR-MSI (used):', M_BANDS)\n",
    "\n",
    "hrsize = CONFIG['PATCH_HR_SIZE']\n",
    "stride = CONFIG['PATCH_STRIDE']\n",
    "num_filter = 32  # keep consistent with model cell\n",
    "msi_bands = M_BANDS\n",
    "hsi_bands = H_BANDS\n",
    "scale = SCALE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oZzq3xoA7mpE"
   },
   "source": [
    "# Build Training Patch Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BSQxDx_gDBpI"
   },
   "outputs": [],
   "source": [
    "# Construct aligned patch tensors (mrdata: HR-MSI, lrdata: upsampled LR-HSI, hrdata: HR-HSI GT) with progress bars\n",
    "start_time = time.time()\n",
    "patches_hr = []\n",
    "patches_lr = []\n",
    "patches_mr = []\n",
    "scene_patch_counts = {}\n",
    "\n",
    "for hr_path, lr_path, msi_path in tqdm(list(zip(train_hr_hsi_files, train_lr_hsi_files, train_hr_msi_files)), desc='Scenes (train)'):\n",
    "    hr_cube,_ = load_first_cube(hr_path)\n",
    "    lr_cube,_ = load_first_cube(lr_path)\n",
    "    msi_cube,_ = load_first_cube(msi_path)\n",
    "    if CONFIG['MSI_BANDS_SELECT']:\n",
    "        msi_cube = msi_cube[:,:,CONFIG['MSI_BANDS_SELECT']]\n",
    "    up_lr = cv.resize(lr_cube, (hr_cube.shape[1], hr_cube.shape[0]), interpolation=cv.INTER_CUBIC)\n",
    "    h, w, _ = hr_cube.shape\n",
    "    row_starts = np.arange(0, h - hrsize, stride)\n",
    "    col_starts = np.arange(0, w - hrsize, stride)\n",
    "    c_before = len(patches_hr)\n",
    "    for i in row_starts:\n",
    "        for j in col_starts:\n",
    "            patches_hr.append(hr_cube[i:i+hrsize, j:j+hrsize, :])\n",
    "            patches_lr.append(up_lr[i:i+hrsize, j:j+hrsize, :])\n",
    "            patches_mr.append(msi_cube[i:i+hrsize, j:j+hrsize, :])\n",
    "    scene_patch_counts[os.path.basename(hr_path)] = len(patches_hr) - c_before\n",
    "\n",
    "hrdata = np.stack(patches_hr, axis=0)\n",
    "lrdata = np.stack(patches_lr, axis=0)\n",
    "mrdata = np.stack(patches_mr, axis=0)\n",
    "elapsed = time.time()-start_time\n",
    "print(f'Patch extraction finished in {elapsed:.1f}s')\n",
    "print('Training patches shapes -> hrdata:', hrdata.shape, 'lrdata:', lrdata.shape, 'mrdata:', mrdata.shape)\n",
    "print('Patches per scene (first 5):', list(scene_patch_counts.items())[:5])\n",
    "\n",
    "# Quick visualization\n",
    "n = min(10, hrdata.shape[0]-1)\n",
    "i = hrdata.shape[3]//2\n",
    "p = np.hstack((hrdata[n,:,:,i], lrdata[n,:,:,i], mrdata[n,:,:,0]))\n",
    "fig, ax = plt.subplots(figsize=(6,6))\n",
    "pyplot.imshow(p, vmin=0, vmax=1, cmap='gray')\n",
    "pyplot.title('HR | UpLR | MSI(first band)')\n",
    "pyplot.axis('off')\n",
    "plt.show()\n",
    "\n",
    "print('Memory usage (approx) HR patches MB:', hrdata.nbytes/1e6)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XdiGN0_qs52k"
   },
   "outputs": [],
   "source": [
    "# Placeholder cell retained (was sample visualization) -- already visualized above.\n",
    "print('Sample mid training patch stats:')\n",
    "print('HR mean', hrdata.mean(), 'LR mean', lrdata.mean(), 'MR mean', mrdata.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J2hRM89f7y5q"
   },
   "source": [
    "# Model (Unmodified BASFE Architecture)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YzUMjoVTvWNG"
   },
   "outputs": [],
   "source": [
    "num_filter = 32\n",
    "\n",
    "def spec(inputs,nf):\n",
    "    x = tf.keras.layers.Conv2D(nf, 1, activation=\"PReLU\", padding=\"same\", use_bias=True)(inputs)\n",
    "    x = tf.keras.layers.Conv2D(nf, 1, activation=\"PReLU\", padding=\"same\", use_bias=True)(x)\n",
    "    return inputs + x\n",
    "def spat(inputs,nf):\n",
    "    x = tf.keras.layers.Conv2D(nf, 3, activation=\"PReLU\", padding=\"same\", use_bias=True)(inputs)\n",
    "    x = tf.keras.layers.Conv2D(nf, 3, activation=\"PReLU\", padding=\"same\", use_bias=True)(x)\n",
    "    return inputs + x\n",
    "\n",
    "# ****************************** MSI Encoder ******************************\n",
    "msi_input = keras.Input(shape=(hrsize, hrsize, msi_bands), name=\"msi_input\")\n",
    "x01 = layers.Conv2D(num_filter, 3, activation=\"PReLU\", padding=\"same\", use_bias=True)(msi_input)\n",
    "x02 = spec(x01,num_filter)\n",
    "x02 = spat(x02,num_filter)\n",
    "x03 = layers.concatenate([x01, x02])\n",
    "\n",
    "x04 = layers.Conv2D(num_filter, 3, activation=\"PReLU\", padding=\"same\", use_bias=True)(x03)\n",
    "x05 = spec(x04,num_filter)\n",
    "x05 = spat(x05,num_filter)\n",
    "x06 = layers.concatenate([x01, x04, x05])\n",
    "\n",
    "x07 = layers.Conv2D(num_filter, 5, activation=\"PReLU\", padding=\"same\", use_bias=True)(x06)\n",
    "x07 = spec(x07,num_filter)\n",
    "x08 = spat(x07,num_filter)\n",
    "\n",
    "# ****************************** LR_HSI Encoder ******************************\n",
    "lr_input = keras.Input(shape=(hrsize, hrsize,hsi_bands), name=\"lr_input\")\n",
    "x11 = layers.Conv2D(num_filter, 3, activation=\"PReLU\", padding=\"same\", use_bias=True)(lr_input)\n",
    "x12 = spec(x11,num_filter)\n",
    "x12 = spat(x12,num_filter)\n",
    "x13 = layers.concatenate([x11, x12])\n",
    "\n",
    "x14 = layers.Conv2D(num_filter, 3, activation=\"PReLU\", padding=\"same\", use_bias=True)(x13)\n",
    "x15 = spec(x14,num_filter)\n",
    "x15 = spat(x15,num_filter)\n",
    "x16 = layers.concatenate([x11, x14, x15])\n",
    "\n",
    "x17 = layers.Conv2D(num_filter, 5, activation=\"PReLU\", padding=\"same\", use_bias=True)(x16)\n",
    "x17 = spec(x17,num_filter)\n",
    "x18 = spat(x17,num_filter)\n",
    "\n",
    "# ********************************* concat ***********************************\n",
    "x21 = layers.concatenate([x01, x04, x07, x08, x11, x14, x17, x18])\n",
    "\n",
    "x22 = layers.Conv2D(hsi_bands, 3, activation=\"PReLU\", padding=\"same\", use_bias=True)(x21)\n",
    "fuse_output = layers.Conv2D(hsi_bands, 3, activation=\"PReLU\", padding=\"same\", use_bias=True, name=\"fuse_output\")(x22)\n",
    "\n",
    "model = keras.Model(\n",
    "    inputs=[msi_input, lr_input],\n",
    "    outputs=[fuse_output],\n",
    ")\n",
    "model.summary()\n",
    "keras.utils.plot_model(model, \"multi_input_and_output_model.png\", show_shapes=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pfk9z5gW74Gy"
   },
   "source": [
    "# Training BASFE on CAVE-style Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kF49KV5v25Gl"
   },
   "outputs": [],
   "source": [
    "# Save initial (untrained) model\n",
    "model.save(CONFIG['SAVE_MODEL_PATH'])\n",
    "\n",
    "# Callbacks for richer logging\n",
    "callbacks = []\n",
    "callbacks.append(keras.callbacks.CSVLogger(CONFIG['CSV_LOG'], append=False))\n",
    "callbacks.append(keras.callbacks.ModelCheckpoint(\n",
    "    filepath=os.path.join(CONFIG['CHECKPOINT_DIR'], 'epoch_{epoch:03d}_loss_{loss:.5f}.keras'),\n",
    "    monitor='loss', save_best_only=True, save_weights_only=False, verbose=1))\n",
    "callbacks.append(keras.callbacks.EarlyStopping(monitor='loss', patience=CONFIG['EARLY_STOP_PATIENCE'], restore_best_weights=True, verbose=1))\n",
    "callbacks.append(keras.callbacks.TensorBoard(log_dir=CONFIG['LOG_DIR'], write_graph=False, update_freq='epoch'))\n",
    "\n",
    "model.compile(\n",
    "    optimizer=tf.optimizers.Adam(learning_rate=CONFIG['LEARNING_RATE']),\n",
    "    loss=keras.losses.MeanSquaredError(),\n",
    ")\n",
    "\n",
    "print('Starting training: epochs', CONFIG['EPOCHS'], 'batch size', CONFIG['BATCH_SIZE'])\n",
    "start_train = time.time()\n",
    "history = model.fit(\n",
    "    {\"msi_input\": mrdata, \"lr_input\": lrdata},\n",
    "    {\"fuse_output\": hrdata},\n",
    "    epochs=CONFIG['EPOCHS'],\n",
    "    batch_size=CONFIG['BATCH_SIZE'],\n",
    "    shuffle=True,\n",
    "    verbose=1,\n",
    "    callbacks=callbacks,\n",
    ")\n",
    "train_time = time.time()-start_train\n",
    "print(f'Training completed in {train_time/60:.2f} min')\n",
    "\n",
    "model.save(CONFIG['SAVE_MODEL_TRAINED_PATH'])\n",
    "\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.plot(history.history['loss'], label='loss')\n",
    "plt.xlabel('Epoch'); plt.ylabel('MSE Loss'); plt.title('Training Loss Curve'); plt.grid(True); plt.legend();\n",
    "plt.tight_layout(); plt.show()\n",
    "\n",
    "# Summary JSON\n",
    "summary_info = {\n",
    "    'epochs_ran': len(history.history['loss']),\n",
    "    'final_loss': float(history.history['loss'][-1]),\n",
    "    'train_time_sec': train_time,\n",
    "    'num_patches': int(hrdata.shape[0]),\n",
    "    'scale': scale,\n",
    "    'hr_patch_size': hrsize,\n",
    "    'bands_hsi': hsi_bands,\n",
    "    'bands_msi': msi_bands,\n",
    "}\n",
    "with open(os.path.join(CONFIG['RESULTS_DIR'], 'training_summary.json'), 'w') as f:\n",
    "    json.dump(summary_info, f, indent=2)\n",
    "print('Saved training summary to training_summary.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A7eycocrLFxx"
   },
   "source": [
    "# Reconstruction & Assessment on Test Scenes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0Gnydw_G8B-2"
   },
   "outputs": [],
   "source": [
    "# Prepare test inputs and run model prediction over each scene via tiling reconstruction with progress indicators.\n",
    "\n",
    "def prepare_test_scene(lr_path, msi_path):\n",
    "    lr_cube,_ = load_first_cube(lr_path)\n",
    "    msi_cube,_ = load_first_cube(msi_path)\n",
    "    if CONFIG['MSI_BANDS_SELECT']:\n",
    "        msi_cube = msi_cube[:,:,CONFIG['MSI_BANDS_SELECT']]\n",
    "    up_lr = cv.resize(lr_cube, (msi_cube.shape[1], msi_cube.shape[0]), interpolation=cv.INTER_CUBIC)\n",
    "    return up_lr, msi_cube\n",
    "\n",
    "TEST_LR = list_mats(CONFIG['TEST_LR_HSI_DIR'])\n",
    "TEST_MSI = list_mats(CONFIG['TEST_HR_MSI_DIR'])\n",
    "if CONFIG['MAX_TEST_SCENES']:\n",
    "    TEST_LR = TEST_LR[:CONFIG['MAX_TEST_SCENES']]\n",
    "    TEST_MSI = TEST_MSI[:CONFIG['MAX_TEST_SCENES']]\n",
    "assert len(TEST_LR)==len(TEST_MSI), 'Mismatch in test file counts'\n",
    "\n",
    "EDGE = CONFIG['EDGE_OVERLAP']\n",
    "strider = hrsize - 2*EDGE\n",
    "\n",
    "reconstructed = {}\n",
    "\n",
    "for lr_path, msi_path in tqdm(list(zip(TEST_LR, TEST_MSI)), desc='Scenes (test)'):\n",
    "    scene_name = os.path.splitext(os.path.basename(lr_path))[0]\n",
    "    up_lr, hr_msi = prepare_test_scene(lr_path, msi_path)\n",
    "    H, W, _ = hr_msi.shape\n",
    "    ii = np.arange(0, H, strider)\n",
    "    jj = np.arange(0, W, strider)\n",
    "    if ii[-1] + hrsize > H: ii[-1] = H - hrsize\n",
    "    if len(ii) > 1 and ii[-2] >= ii[-1]: ii = ii[:-1]\n",
    "    if jj[-1] + hrsize > W: jj[-1] = W - hrsize\n",
    "    if len(jj) > 1 and jj[-2] >= jj[-1]: jj = jj[:-1]\n",
    "\n",
    "    mr_patches = []\n",
    "    lr_patches = []\n",
    "    for i in ii:\n",
    "        for j in jj:\n",
    "            mr_patches.append(hr_msi[i:i+hrsize, j:j+hrsize, :])\n",
    "            lr_patches.append(up_lr[i:i+hrsize, j:j+hrsize, :])\n",
    "    mrdatainput = np.stack(mr_patches, axis=0)\n",
    "    lrdatainput = np.stack(lr_patches, axis=0)\n",
    "\n",
    "    preds = model.predict((mrdatainput, lrdatainput), verbose=0)\n",
    "\n",
    "    out_cube = np.zeros((H, W, hsi_bands), dtype=np.float32)\n",
    "    count = 0\n",
    "    for i in ii:\n",
    "        for j in jj:\n",
    "            out_cube[i:i+hrsize, j:j+hrsize, :] = preds[count]\n",
    "            count += 1\n",
    "    count = 0\n",
    "    for i in ii:\n",
    "        for j in jj:\n",
    "            out_cube[i+EDGE:i+hrsize-EDGE, j+EDGE:j+hrsize-EDGE, :] = preds[count, EDGE:-EDGE, EDGE:-EDGE, :]\n",
    "            count += 1\n",
    "\n",
    "    reconstructed[scene_name] = out_cube\n",
    "    savemat(os.path.join(CONFIG['RESULTS_DIR'], f'{scene_name}_reconst.mat'), {f'reconst_{scene_name}': out_cube})\n",
    "    print(f'Reconstructed {scene_name}: shape {out_cube.shape}')\n",
    "\n",
    "print('Total test scenes reconstructed:', len(reconstructed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0RbcjOqCnyda"
   },
   "outputs": [],
   "source": [
    "# Metrics (per-scene if GT available) + consolidated logging\n",
    "from math import log10\n",
    "\n",
    "metrics_results = {}\n",
    "\n",
    "if CONFIG['TEST_GT_HR_HSI_DIR']:\n",
    "    gt_files = list_mats(CONFIG['TEST_GT_HR_HSI_DIR'])\n",
    "    gt_map = {os.path.splitext(os.path.basename(f))[0]: f for f in gt_files}\n",
    "\n",
    "    def compute_metrics(pred, gt, scale):\n",
    "        assert pred.shape == gt.shape\n",
    "        z = pred.shape\n",
    "        n = z[0]*z[1]\n",
    "        L = z[2]\n",
    "        temp = np.sum(np.sum((pred-gt)*(pred-gt),axis=0),axis=0)/n\n",
    "        rmse_per_band = np.sqrt(temp)\n",
    "        rmse_total = np.sqrt(np.sum(temp)/L)\n",
    "        psnr = 10*log10(1.0/(rmse_total**2 + 1e-12))\n",
    "        num = np.sum(pred*gt,axis=2)\n",
    "        den = np.sqrt(np.sum(pred*pred,axis=2)*np.sum(gt*gt,axis=2))+1e-12\n",
    "        sam = np.mean(np.arccos(np.clip(num/den, -1, 1))) * 180/np.pi\n",
    "        mean_gt = np.sum(np.sum(gt,axis=0),axis=0)/n\n",
    "        ergas = 100/scale*np.sqrt(np.sum((rmse_per_band / (mean_gt+1e-12))**2)/L)\n",
    "        c1=.0001; c2=.0001\n",
    "        ssim = []\n",
    "        mean_p = np.sum(np.sum(pred,axis=0),axis=0)/n\n",
    "        for i in range(L):\n",
    "            sigma2_p = np.mean(pred[:,:,i]**2)-mean_p[i]**2\n",
    "            sigma2_g = np.mean(gt[:,:,i]**2)-mean_gt[i]**2\n",
    "            sigma_pg = np.mean(pred[:,:,i]*gt[:,:,i]) - mean_p[i]*mean_gt[i]\n",
    "            ssim_i = ((2*mean_p[i]*mean_gt[i]+c1)*(2*sigma_pg+c2))/((mean_p[i]**2+mean_gt[i]**2+c1)*(sigma2_p+sigma2_g+c2))\n",
    "            ssim.append(ssim_i)\n",
    "        mssim = float(np.mean(ssim))\n",
    "        cc = []\n",
    "        for i in range(L):\n",
    "            cc_num = np.sum((pred[:,:,i]-mean_p[i])*(gt[:,:,i]-mean_gt[i]))\n",
    "            cc_den = np.sqrt(np.sum((pred[:,:,i]-mean_p[i])**2)*np.sum((gt[:,:,i]-mean_gt[i])**2))+1e-12\n",
    "            cc.append(cc_num/cc_den)\n",
    "        return {\n",
    "            'RMSE': float(rmse_total),\n",
    "            'PSNR': float(psnr),\n",
    "            'SAM_deg': float(sam),\n",
    "            'ERGAS': float(ergas),\n",
    "            'MSSIM': float(np.mean(ssim)),\n",
    "            'CC': float(np.mean(cc)),\n",
    "        }\n",
    "\n",
    "    for scene, pred in reconstructed.items():\n",
    "        if scene in gt_map:\n",
    "            gt_cube,_ = load_first_cube(gt_map[scene])\n",
    "            H = min(gt_cube.shape[0], pred.shape[0])\n",
    "            W = min(gt_cube.shape[1], pred.shape[1])\n",
    "            gt_crop = gt_cube[:H,:W,:pred.shape[2]]\n",
    "            pred_crop = pred[:H,:W,:pred.shape[2]]\n",
    "            metrics_results[scene] = compute_metrics(pred_crop, gt_crop, scale)\n",
    "            print('Metrics', scene, metrics_results[scene])\n",
    "else:\n",
    "    print('No GT directory configured for test metrics; skipping metric computation.')\n",
    "\n",
    "# Save metrics JSON if any\n",
    "os.makedirs(CONFIG['RESULTS_DIR'], exist_ok=True)\n",
    "if metrics_results:\n",
    "    with open(os.path.join(CONFIG['RESULTS_DIR'], 'metrics.json'), 'w') as f:\n",
    "        json.dump(metrics_results, f, indent=2)\n",
    "    print('Saved metrics.json')\n",
    "print('Done.')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
